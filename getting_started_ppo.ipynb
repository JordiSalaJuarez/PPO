{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "if not Path(\"utils.py\").exists():\n",
    "    !wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install procgen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_steps = 8e6\n",
    "num_envs = 32\n",
    "num_levels = 10\n",
    "num_steps = 256\n",
    "num_epochs = 3\n",
    "batch_size = 512\n",
    "eps = .2\n",
    "grad_eps = .5\n",
    "value_coef = .5\n",
    "entropy_coef = .01\n",
    "\n",
    "# other config. Values obtained from https://github.com/DarylRodrigo/rl_lib\n",
    "in_channels = 3\n",
    "feature_dim = 64\n",
    "entropy_beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import make_env, Storage, orthogonal_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6724/1549892580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Iterate over batches of transitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0mb_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_log_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_advantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/projects/ppo/utils.py\u001b[0m in \u001b[0;36mget_generator\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/ppo-vrOzCtYD-py3.8/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels, feature_dim):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "        Flatten(),\n",
    "        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
    "    )\n",
    "    self.apply(orthogonal_init)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "  def __init__(self, encoder, feature_dim, num_actions):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
    "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
    "\n",
    "  def act(self, x):\n",
    "    with torch.no_grad():\n",
    "      x = x.cuda().contiguous()\n",
    "      dist, value, entropy = self.forward(x)\n",
    "      action = dist.sample()\n",
    "      log_prob = dist.log_prob(action)\n",
    "    \n",
    "    return action, log_prob, value, entropy\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    logits = self.policy(x)\n",
    "    value = self.value(x).squeeze(1)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    return dist, value, dist.entropy()\n",
    "\n",
    "\n",
    "# Define environment\n",
    "# check the utils.py file for info on arguments\n",
    "env = make_env(num_envs, num_levels=num_levels)\n",
    "# print('Observation space:', env.observation_space)\n",
    "# print('Action space:', env.action_space.n)\n",
    "\n",
    "# Define network\n",
    "encoder = Encoder(in_channels, feature_dim)\n",
    "policy = Policy(encoder, feature_dim, env.action_space.n)\n",
    "policy.cuda()\n",
    "\n",
    "# Define optimizer\n",
    "# these are reasonable values but probably not optimal\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
    "\n",
    "# Define temporary storage\n",
    "# we use this to collect transitions during each iteration\n",
    "storage = Storage(\n",
    "    env.observation_space.shape,\n",
    "    num_steps,\n",
    "    num_envs\n",
    ")\n",
    "\n",
    "# Run training\n",
    "obs = env.reset()\n",
    "step = 0\n",
    "while step < total_steps:\n",
    "\n",
    "  # Use policy to collect data for num_steps steps\n",
    "  policy.eval()\n",
    "  for _ in range(num_steps):\n",
    "    # Use policy\n",
    "    action, log_prob, value, entropy = policy.act(obs)\n",
    "    \n",
    "    # Take step in environment\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # Store data\n",
    "    storage.store(obs, action, reward, done, info, log_prob, value)\n",
    "    \n",
    "    # Update current observation\n",
    "    obs = next_obs\n",
    "\n",
    "  # Add the last observation to collected data\n",
    "  _, _, value, _ = policy.act(obs)\n",
    "  storage.store_last(obs, value)\n",
    "\n",
    "  # Compute return and advantage\n",
    "  storage.compute_return_advantage()\n",
    "\n",
    "  # Optimize policy\n",
    "  policy.train()\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate over batches of transitions\n",
    "    generator = storage.get_generator(batch_size)\n",
    "    for batch in generator:\n",
    "      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
    "\n",
    "      # Get current policy outputs\n",
    "      new_dist, new_value, entropy = policy(b_obs)\n",
    "      actions, log_probs, _, entropy = policy.act(b_obs)\n",
    "      new_log_prob = new_dist.log_prob(b_action)\n",
    "\n",
    "      # Clipped policy objective\n",
    "      # calculate surrogates\n",
    "      ratio = torch.exp(log_probs - b_log_prob)\n",
    "      surrogate_1 = b_advantage * ratio\n",
    "      surrogate_2 = b_advantage * torch.clamp(ratio, 1-eps, 1+eps)\n",
    "      pi_loss = -torch.min(surrogate_1, surrogate_2)  # Policy gradient objective, also L^{PG} or PG loss\n",
    "\n",
    "      # Clipped value function objective\n",
    "      value_loss_unclipped = (new_value - b_returns)**2\n",
    "      values_clipped = b_value + torch.clamp(new_value - b_value, -eps, eps)\n",
    "      value_loss_clipped = (values_clipped - b_returns)**2\n",
    "      value_loss =  0.5 * torch.mean(torch.max(value_loss_clipped, value_loss_unclipped))\n",
    "\n",
    "      # Entropy loss\n",
    "      entropy_loss = entropy.mean()\n",
    "\n",
    "      # Backpropagate losses\n",
    "      loss = pi_loss + value_loss - entropy_beta*entropy_loss # as defined at https://github.com/DarylRodrigo/rl_lib/blob/f165aabb328cb5c798360640fcef58792a72ae8a/PPO/PPO.py#L97\n",
    "      loss.sum().backward()\n",
    "\n",
    "      # Clip gradients\n",
    "      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
    "\n",
    "      # Update policy\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "  # Update stats\n",
    "  step += num_envs * num_steps\n",
    "  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
    "\n",
    "print('Completed training!')\n",
    "torch.save(policy.state_dict, 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# Make evaluation environment\n",
    "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels)\n",
    "obs = eval_env.reset()\n",
    "\n",
    "frames = []\n",
    "total_reward = []\n",
    "\n",
    "# Evaluate policy\n",
    "policy.eval()\n",
    "for _ in range(512):\n",
    "\n",
    "  # Use policy\n",
    "  action, log_prob, value = policy.act(obs)\n",
    "\n",
    "  # Take step in environment\n",
    "  obs, reward, done, info = eval_env.step(action)\n",
    "  total_reward.append(torch.Tensor(reward))\n",
    "\n",
    "  # Render environment and store\n",
    "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
    "  frames.append(frame)\n",
    "\n",
    "# Calculate average return\n",
    "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
    "print('Average return:', total_reward)\n",
    "\n",
    "# Save frames as video\n",
    "frames = torch.stack(frames)\n",
    "imageio.mimsave('vid.mp4', frames, fps=25)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce842456d028b977e97ff52c6c7d50ba37e38f708af23e2ef83d803d21d0100d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
